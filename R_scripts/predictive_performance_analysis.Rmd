---
title: "Analysis of predictive performance"
output:
  html_notebook: default
  pdf_document: default
---

# Analysis of predictive performance

All simulations run with the DrugLogics pipeline are analysed in terms of predictive performance.

### Import necessary packages

```{r}
suppressMessages(library(dplyr))
suppressMessages(library(igraph))
suppressMessages(library(Matrix))
suppressMessages(library(gtools))
suppressMessages(library(tibble))
suppressMessages(library(emba))
suppressMessages(library(usefun))
suppressMessages(library(PRROC))
suppressMessages(library(DT))
suppressMessages(library(readr))
suppressMessages(library(ggplot2))
suppressMessages(library(pheatmap))
suppressMessages(library(stringr))
```

### Define relevant variable names

The file including all node activities is imported and stored as a data frame including two columns with node names and corresponding activity values.

```{r}
model_cl = readline(prompt="Enter model and cell line identifier: ")
nodes_num = as.integer(readline(prompt="Enter number of nodes in network (excluding output nodes): "))
pipeline_folder = readline(prompt="Enter the name of the pipeline folder: ")
random_model_result = readline(prompt="Enter the name of the result folder from the random proliferation simulation: ")
random_model_result = readline(prompt="Enter the name of the result folder for the random model: ")
```

# Calculating predictive performance

### *calculate_predictions():* Calculating AUC ROC for a selection of synergy simulations

Function is used to calculate the ensemble-wise AUC ROC for a selection of synergy simulations. 
The model+cell line identifier as well as the mode (random, DP, Z) is given as input argument to the function. 
The output of the function is a dataframe including all simulation names, sample sizes and AUC ROC values.

```{r}
calculate_predictions = function(model_cl, mode){
  #Make a list of all folders with simulation results
  path_to_results = paste0('druglogics-synergy/', pipeline_folder)
  simulation_results = mixedsort(dir(path=path_to_results, pattern = paste0(mode, '_', model_cl)))
  
  size_list = c()
  ROC_list = c()
  simulation_list = c()
  
  for (i in simulation_results){
    i_short = substr(i, 1, nchar(i)-15)
    # Read ensemble-wise synergies file and get the synergy scores
    calib_model_file = paste0(path_to_results, '/', i, '/', i_short, 'ensemblewise_synergies.tab')
    calib_model_ensemblewise_synergies = emba::get_synergy_scores(calib_model_file)
    
    # Read observed synergies file
    observed_synergies_file = paste0(path_to_results, '/observed_synergies_', model_cl)
    observed_synergies = emba::get_observed_synergies(observed_synergies_file)
    # 1 (positive/observed synergy) or 0 (negative/not observed) for all tested drug combinations
    observed = sapply(calib_model_ensemblewise_synergies$perturbation %in% observed_synergies, as.integer)
    
    synergy_pred_table = dplyr::bind_cols(calib_model_ensemblewise_synergies %>% rename(ss_score = score),
                              tibble::as_tibble_col(observed, column_name = "observed"))
  
    # Get ROC statistics (`roc_res$AUC` holds the ROC AUC)
    roc_res = usefun::get_roc_stats(df = synergy_pred_table, pred_col = "ss_score", label_col = "observed")
    
    # Read ensemble-wise synergies file for random model simulation
    random_model_file = paste0(path_to_results, '/', random_model_result, '/', random_model_result, '_ensemblewise_synergies.tab')
    random_model_ensemblewise_synergies = emba::get_synergy_scores(random_model_file)
  
    # check: predictions for the same perturbations
    stopifnot(all(random_model_ensemblewise_synergies$perturbation == calib_model_ensemblewise_synergies$perturbation))
  
    # Add random predictions column to the predictions table
    synergy_pred_table = synergy_pred_table %>%
    tibble::add_column(random_score = random_model_ensemblewise_synergies$score, .before = 'observed')
    
    #Add the normalized synergy score to the table
    synergy_pred_table = synergy_pred_table %>%
    mutate(norm_score = ss_score - random_score, .before = 'observed')
  
    # Get ROC statistics (`roc_res_norm$AUC` holds the ROC AUC)
    roc_res_norm = usefun::get_roc_stats(df = synergy_pred_table, pred_col = "norm_score", label_col = "observed")
    
    #Get sample size
    s = parse_number(substr(i_short, nchar(i_short)-3, nchar(i_short)-1))
    
    #Add sample size, simulation name and AUC ROC value to respective lists
    size_list = append(size_list, s)
    ROC_list = append(ROC_list, roc_res_norm$AUC)
    simulation_list = append(simulation_list, i)
  }
  
  #Create a dataframe which holds all the results (simulation name, sample size and AUC ROC value)
  ROC_AUC_table = data.frame(simulation_list, size_list, ROC_list)
  return(ROC_AUC_table)
}
```

```{r}
DP_predictions = calculate_predictions(model_cl,'DP')
Z_predictions = calculate_predictions(model_cl, 'Z')
random_predictions = calculate_predictions(model_cl,'random')
```

### Regression analysis: Assess the association between sample size and AUC ROC

```{r}
random_lm = lm(random_predictions$ROC_list~random_predictions$size_list)
summary(random_lm)
```

### Plotting the results

Boxplots are used to visualise results.

```{r}
#Random sample results

random_predictions$size_list = as.character(random_predictions$size_list)
random_predictions$size_list = factor(random_predictions$size_list, levels = c('10', '20', '30', '40', '50', '60', '70', '80', '90', '100'))

random_boxplot = ggplot(data=random_predictions, aes(x=size_list, y=ROC_list, fill = size_list))+
  geom_boxplot(show.legend=FALSE)+
  scale_fill_brewer(palette = 'Paired')+
  geom_jitter(shape=20, position = position_jitter(0.05), show.legend=FALSE)+
  ylim(c(0,1))+
  labs(x = 'Sample size (% of nodes from baseline activity profile)', y = 'AUC ROC', title = "Predictive performance of models calibrated to \nrandom sample calibration datasets")+
  theme(text = element_text(size = 14, family = "serif"))+
  geom_hline(yintercept = 0.5, linetype = 'dashed', color = "red") +
  geom_text(aes(x = 10, y = 0.45, label = "RCL"), size = 5, family='serif')

print(random_boxplot)
```

#### Identify interqartile (IQR)

```{r}
random_IQR = list(boxplot(random_predictions$ROC_list ~ random_predictions$size_list))
random_IQR[[1]]$stats
```

```{r}
#DP-based results

DP_predictions$size_list = as.character(DP_predictions$size_list)
DP_predictions$size_list = factor(DP_predictions$size_list, levels = c('10', '20', '30', '40', '50', '60', '70', '80', '90', '100'))

DP_boxplot = ggplot(data=random_predictions, aes(x=size_list, y=ROC_list, fill = size_list))+
  geom_boxplot(show.legend=FALSE)+
  scale_fill_brewer(palette = 'Paired')+
  geom_jitter(shape=20, position = position_jitter(0.05), show.legend=FALSE)+
  ylim(c(0,1))+
  labs(x = 'Sample size (% of nodes from baseline activity profile)', y = 'AUC ROC', title = "Predictive performance of models calibrated to random sample \nand DP-based calibration datasets")+
  theme(text = element_text(size = 14, family = "serif"))+
  geom_hline(yintercept = 0.5, linetype = 'dashed', color = "red") +
  geom_text(aes(x = 10, y = 0.45, label = "RCL"), size = 5, family='NimbusSan')+
  geom_point(data=DP_predictions, aes(x=size_list,y=ROC_list), colour='red', size=2.5, show.legend = FALSE)

print(DP_boxplot)
```

```{r}
#Z-based results

Z_predictions$size_list = as.character(Z_predictions$size_list)
Z_predictions$size_list = factor(Z_predictions$size_list, levels = c('10', '20', '30', '40', '50', '60', '70', '80', '90', '100'))

Z_boxplot = ggplot(data=random_predictions, aes(x=size_list, y=ROC_list, fill = size_list))+
  geom_boxplot(show.legend=FALSE)+
  scale_fill_brewer(palette = 'Paired')+
  geom_jitter(shape=20, position = position_jitter(0.05), show.legend=FALSE)+
  ylim(c(0,1))+
  labs(x = 'Sample size (% of nodes from baseline activity profile)', y = 'AUC ROC', title = "Predictive performance of models calibrated to random sample \nand Z-score-based calibration datasets")+
  theme(text = element_text(size = 14, family = "serif"))+
  geom_hline(yintercept = 0.5, linetype = 'dashed', color = "red") +
  geom_text(aes(x = 10, y = 0.45, label = "RCL"), size = 5, family='NimbusSan')+
  geom_point(data=Z_predictions, aes(x=size_list,y=ROC_list), colour='red', size=2.5, show.legend = FALSE)

print(Z_boxplot)
```

```{r}
#Calculate AUC ROC value for simulation using calibration data based on the near-minimal FVS
FVS_predictions = calculate_predictions(model_cl,'FVS')

FVS_size = as.integer(readline(prompt="Enter number of nodes included in the FVS based calibration dataset: "))
FVS_predictions$size_list = round(100/nodes_num * FVS_size)

FVS_sample_size = FVS_predictions$size_list
FVS_AUC_ROC = FVS_predictions$ROC_list[1]

print(paste0('The sample size of the near-minimal FVS: ', FVS_sample_size, "%"))
print(paste0('The AUC ROC value of the FVS based simulation: ', FVS_AUC_ROC))
```

#Correlation between fitness and AUC ROC

```{r}
#A function to retrieve the average fitness across 150 models in one simulation. This is calculated for all simulations using calibration data of a certain mode
get_fitness = function(mode){
  Simulations = mixedsort(dir(path=path_to_results,pattern = paste0(mode,'_',model_cl)))
  
  fitness_table = data.frame()
  
  for (simulation in Simulations){
    simulation_short = substr(simulation,1,nchar(simulation)-15)
    models_dir = paste0(path_to_results,'/',simulation,"/models")
    model_names = mixedsort((dir(path=models_dir, pattern = 'gitsbe')))
    fitness_sum = 0
    for (model in model_names){
      fitness = readLines(paste0(models_dir,'/',model), n = 3)
      fitness = as.double(substr(fitness[3],10,nchar(fitness[3])))
      fitness_sum = fitness_sum + fitness
    }
    average_fitness = fitness_sum/length(model_names)
    size = parse_number(substr(simulation_short,nchar(simulation_short)-3,nchar(simulation_short)-1))
    sim_result = data.frame(simulation, size, average_fitness)
    fitness_table = rbind(fitness_table, sim_result)
  }
  return(fitness_table)
}
```

```{r}
#Fitness for simulations using random sample calibration data
random_fitness = get_fitness('random')
rownames(random_predictions) = random_predictions$simulation_list
rownames(random_fitness) = random_fitness$simulation
random_ROC_fitness_table = cbind(random_predictions, random_fitness)

#Plot calibration dataset size against average fitness score
ggplot()+
  geom_point(data = random_fitness, aes(x = size, y = average_fitness))+
  labs(title = "Average fitness of 150 models generated by pipeline simulations \nusing random sample calibration datasets")+
  ylab("Average fitness across 150 models")+
  xlab("Calibration datasets size (% of total node number)")+
  ylim(0.5,1.0)+
  xlim(0,100)
```

```{r}
#Fitness for simulations using calibration data based on DP
DP_fitness = get_fitness('DP')
rownames(DP_predictions) = DP_predictions$simulation_list
rownames(DP_fitness) = DP_fitness$simulation
DP_ROC_fitness_table = cbind(DP_predictions, DP_fitness)

#Plot calibration dataset size against average fitness score
ggplot()+
  geom_point(data = DP_fitness, aes(x = size, y = average_fitness))+
  labs(title = "Average fitness of 150 models generated by pipeline simulations \nusing calibration datasets based on DP")+
  ylab("Average fitness across 150 models")+
  xlab("Calibration datasets size (% of total node number)")+
  ylim(0.5,1.0)+
  xlim(0,100)
```

```{r}
#Fitness for simulations using calibration data based on Z
Z_fitness = get_fitness('Z')
rownames(Z_predictions) = Z_predictions$simulation_list
rownames(Z_fitness) = Z_fitness$simulation
Z_ROC_fitness_table = cbind(Z_predictions, Z_fitness)

#Plot calibration dataset size against average fitness score
ggplot()+
  geom_point(data = Z_fitness, aes(x = size, y = average_fitness))+
  labs(title = "Average fitness of 150 models generated by pipeline simulations \nusing calibration datasets based on degree Z-score")+
  ylab("Average fitness across 150 models")+
  xlab("Calibration datasets size (% of total node number)")+
  ylim(0.5,1.0)+
  xlim(0,100)
```

```{r}
#Plot average fitness against AUC ROC value to assess the correlation between these two measures
ggplot()+
  geom_point(data = random_ROC_fitness_table, aes(x = average_fitness, y = ROC_list))+
  labs(title = "AUC ROC (performance) vs. average fitness (model fitness) for simulations using \nrandom sample calibration data")+
  xlab("Average fitness across 150 models")+
  ylab("ROC AUC")+
  xlim(0.5,1.0)+
  ylim(0,1)

ggplot()+
  geom_point(data = DP_ROC_fitness_table, aes(x = average_fitness, y = ROC_list))+
  labs(title = "AUC ROC (performance) vs. average fitness (model fitness) for simulations using \ncalibration data based on DP")+
  xlab("Average fitness across 150 models")+
  ylab("ROC AUC")+
  xlim(0.5,1.0)+
  ylim(0,1)

ggplot()+
  geom_point(data = Z_ROC_fitness_table, aes(x = average_fitness, y = ROC_list))+
  labs(title = "AUC ROC (performance) vs. average fitness (model fitness) for simulations using \ncalibration data based on degree Z-score")+
  xlab("Average fitness across 150 models")+
  ylab("ROC AUC")+
  xlim(0.5,1.0)+
  ylim(0,1)

```

```{r}
#Calculate the correlation between fitness and AUC ROC value
print("Performance vs. fitness correlation (kendall) for simulations using random sample calibration datasets: ")
cor(random_ROC_fitness_table$ROC_list, random_ROC_fitness_table$average_fitness, method = "kendall")
print("Performance vs. fitness correlation (kendall) for simulations using calibration datasets based on DP: ")
cor(DP_ROC_fitness_table$ROC_list, DP_ROC_fitness_table$average_fitness, method = "kendall")
print("Performance vs. fitness correlation (kendall) for simulations using calibration datasets based on degree Z-score: ")
cor(Z_ROC_fitness_table$ROC_list, Z_ROC_fitness_table$average_fitness, method = "kendall")
```
